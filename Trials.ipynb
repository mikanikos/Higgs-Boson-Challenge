{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from proj1_helpers import *\n",
    "from costs import *\n",
    "from helpers import *\n",
    "from implementations import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_trials(y, tx, tx_sub, degree_range, lambda_range, partitions=2):\n",
    "    ## Split data into test and training sets\n",
    "    ## If partitions > 2, use k-fold cross-validation\n",
    "    glob_tx_tr, glob_tx_te, glob_y_tr, glob_y_te = split_data(tx, y, 0.8)\n",
    "\n",
    "    ## Initial results: losses, weights, preditions and (test) losses\n",
    "    models = []\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    predictions = []\n",
    "    \n",
    "    ## Loops over range of degrees\n",
    "    degrees = range(degree_range[0], degree_range[1])\n",
    "    lambdas = np.logspace(lambda_range[0], lambda_range[1], num=(lambda_range[1]-lambda_range[0]))\n",
    "    for degree in degrees:\n",
    "        ## Loops over range of lambdas\n",
    "        for lambda_ in lambdas:\n",
    "            print(\"Trying degree\", degree,\"with lambda =\", lambda_,\":\")\n",
    "\n",
    "            tx_tr, tx_te, tx_pred = expand(degree, glob_tx_tr, glob_tx_te, tx_sub)\n",
    "\n",
    "            w, loss = ridge_regression(glob_y_tr, tx_tr, lambda_)\n",
    "            print(\"\\tTraining Loss = \", loss)\n",
    "\n",
    "            y_test = predict_labels(w, tx_te)\n",
    "            test_loss = compute_loss(glob_y_te, tx_te, w)\n",
    "            accuracy = compute_accuracy((y_test+1)/2, glob_y_te)\n",
    "            y_pred = predict_labels(w, tx_pred)\n",
    "\n",
    "            print(\"\\tTest Loss = \", test_loss, \" Test Accuracy = \", accuracy )\n",
    "            models.append((\"ridge_regression\", degree, lambda_, w))\n",
    "            losses.append(test_loss)\n",
    "            accuracies.append(accuracy)\n",
    "            predictions.append(y_pred)\n",
    "    return models, losses, accuracies, predictions\n",
    "    \n",
    "MAX_ITERS = 100  \n",
    "GAMMA = 0.6\n",
    "\n",
    "## Performs logistic trials over set of hyper-parameters (degrees)\n",
    "## Results result from these trials with corresponding test losses\n",
    "def logistic_trials(y, tx, tx_sub, degree_range, partitions=2):\n",
    "    ## Split data into test and training sets\n",
    "    ## If partitions > 2, use k-fold cross-validation\n",
    "    glob_tx_tr, glob_tx_te, glob_y_tr, glob_y_te = split_data(tx, y, 0.8)\n",
    "\n",
    "    ## Initial results: losses, weights, preditions and (test) losses\n",
    "    models = []\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    predictions = []\n",
    "    \n",
    "    ## Loops over range of degrees\n",
    "    degrees = range(degree_range[0], degree_range[1])\n",
    "    for degree in degrees:\n",
    "        print(\"Trying degree\", degree, \":\")\n",
    "\n",
    "        tx_tr, tx_te, tx_pred = expand(degree, glob_tx_tr, glob_tx_te, tx_sub)        \n",
    "        initial_w = np.ones(tx_tr.shape[1])\n",
    "        \n",
    "        w, loss = logistic_SGD(glob_y_tr, tx_tr, initial_w, MAX_ITERS, GAMMA)\n",
    "        print(\"\\tTraining Loss = \", loss)\n",
    "        \n",
    "        y_test = predict_labels(w, tx_te)\n",
    "        test_loss = compute_loss(glob_y_te, tx_te, w, func=\"logistic\")\n",
    "        accuracy = compute_accuracy((y_test+1)/2, glob_y_te)\n",
    "        y_pred = predict_labels(w, tx_pred)\n",
    "\n",
    "        print(\"\\tTest Loss = \", test_loss, \" Test Accuracy = \", accuracy )\n",
    "        models.append((\"logistic_SGD\", degree, w))\n",
    "        losses.append(test_loss)\n",
    "        accuracies.append(accuracy)\n",
    "        predictions.append(y_pred)\n",
    "    return models, losses, accuracies, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Because expansion and standardization are transformations of our initial feature set\n",
    "## We must apply identical transformations to all feature sets we wish to make predictions upon\n",
    "def expand(degree, tx_tr, tx_te, tx_pred):\n",
    "    ## Extract jet numbers as three indicator variables\n",
    "    ## Remove them so they will not be standardized or expanded\n",
    "    jets_tr = jet_nums(tx_tr)\n",
    "    jets_te = jet_nums(tx_te)\n",
    "    jets_pred = jet_nums(tx_pred)\n",
    "    tx_tr = np.delete(tx_tr, 22, axis=1)\n",
    "    tx_te = np.delete(tx_te, 22, axis=1)\n",
    "    tx_pred = np.delete(tx_pred, 22, axis=1)\n",
    "    ## Expand features to include polynomial terms\n",
    "    res_tr = build_poly(tx_tr, degree)\n",
    "    res_te = build_poly(tx_te, degree)\n",
    "    res_pred = build_poly(tx_pred, degree)\n",
    "    ## Standardize\n",
    "    res_tr, mean, std = standardize(res_tr)\n",
    "    res_te = (res_te-mean)/std\n",
    "    res_pred = (res_pred-mean)/std\n",
    "    ## Fix NaNs resulting from division by 0\n",
    "    res_tr[np.isnan(res_tr)]=1\n",
    "    res_te[np.isnan(res_te)]=1\n",
    "    res_pred[np.isnan(res_pred)]=1\n",
    "    ## Reconcatenate jet indicator features\n",
    "    res_tr = np.c_[res_tr, jets_tr]\n",
    "    res_te = np.c_[res_te, jets_te]\n",
    "    res_pred = np.c_[res_pred, jets_pred]\n",
    "    return res_tr, res_te, res_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tx(tx):\n",
    "    result = np.copy(tx)\n",
    "    ## Convert -999 to NaN as we believe these are misidentified data\n",
    "    ## Recording as NaN prevents them from influencing nanmean calculations\n",
    "    result[result==-999]=np.nan\n",
    "    \n",
    "    ## Now replace each NaN index with the mean of its column\n",
    "    means = np.nanmean(result, axis=0)\n",
    "    nans = np.where(np.isnan(result))\n",
    "    result[nans] = np.take(means, nans[1])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Jet number seems to be categorical, taking on three discrete values\n",
    "## Relative values do not seem to have meaning, so coefficients are not a good way to treat this\n",
    "## Solution: Split this into three indicator vectors. Each indicator takes a different coefficient\n",
    "def jet_nums(tx):\n",
    "    jets = tx[:,22]\n",
    "    jet0 = np.zeros(jets.shape)\n",
    "    jet0[jets==0] = 1\n",
    "    jet1 = np.zeros(jets.shape)\n",
    "    jet1[jets==1] = 1\n",
    "    jet2 = np.zeros(jets.shape)\n",
    "    jet2[jets==2] = 1\n",
    "    jet3 = np.zeros(jets.shape)\n",
    "    jet3[jets==3] = 1\n",
    "    result = np.c_[jet0, jet1, jet2, jet3]\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-16d407695c03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Load training sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_csv_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m## For training, let y exist on {0, 1} rather than {-1, 1} to improve cost calculations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ML/proj1/MachineLearning_Project1/proj1_helpers.py\u001b[0m in \u001b[0;36mload_csv_data\u001b[0;34m(data_path, sub_sample)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m\"\"\"Loads data and returns y (class labels), tX (features) and ids (event ids)\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0minput_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mgenfromtxt\u001b[0;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding)\u001b[0m\n\u001b[1;32m   1971\u001b[0m                                    for (v, m) in zip(values,\n\u001b[1;32m   1972\u001b[0m                                                      missing_values)]))\n\u001b[0;32m-> 1973\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax_rows\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1974\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1975\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Load training sets\n",
    "y, tx, ids = load_csv_data(\"data/train.csv\")\n",
    "\n",
    "## For training, let y exist on {0, 1} rather than {-1, 1} to improve cost calculations\n",
    "y = (y + 1) / 2\n",
    "\n",
    "## Fix issues with dataset involving suspect outliers\n",
    "tx = clean_tx(tx)\n",
    "\n",
    "## Load submission dataset\n",
    "y_sub, tx_sub, ids_sub = load_csv_data(\"data/test.csv\")\n",
    "tx_sub = clean_tx(tx_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying degree 1 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marshallcooper/ML/proj1/MachineLearning_Project1/helpers.py:39: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  x = x/stds\n",
      "/Users/marshallcooper/ML/proj1/MachineLearning_Project1/helpers.py:39: RuntimeWarning: invalid value encountered in true_divide\n",
      "  x = x/stds\n",
      "/Users/marshallcooper/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/marshallcooper/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/marshallcooper/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/marshallcooper/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss =  5.814517551023852e-09\n",
      "\tTest Loss =  nan  Test Accuracy =  0.65526\n",
      "Trying degree 2 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marshallcooper/ML/proj1/MachineLearning_Project1/costs.py:18: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = y.T.dot(np.log(pred)) + (1 - y).T.dot(np.log(1 - pred))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss =  0.005028593142453755\n",
      "\tTest Loss =  nan  Test Accuracy =  0.63224\n",
      "Trying degree 3 :\n",
      "\tTraining Loss =  6.0101281917448555\n",
      "\tTest Loss =  nan  Test Accuracy =  0.67258\n",
      "Trying degree 4 :\n",
      "\tTraining Loss =  19.55344188153827\n",
      "\tTest Loss =  nan  Test Accuracy =  0.6444\n",
      "Trying degree 5 :\n",
      "\tTraining Loss =  67.65133676781389\n",
      "\tTest Loss =  nan  Test Accuracy =  0.6064\n"
     ]
    }
   ],
   "source": [
    "models, losses, accuracies, preds =  logistic_trials(y, tx, tx_sub, (1,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_logsgd = preds[np.argmax(accuracies)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1., -1., -1., ...,  1., -1., -1.])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_logsgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying degree 1 with lambda = 0.01 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marshallcooper/ML/proj1/MachineLearning_Project1/helpers.py:39: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  x = x/stds\n",
      "/Users/marshallcooper/ML/proj1/MachineLearning_Project1/helpers.py:39: RuntimeWarning: invalid value encountered in true_divide\n",
      "  x = x/stds\n",
      "/Users/marshallcooper/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/marshallcooper/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/marshallcooper/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/marshallcooper/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss =  0.0844202289678236\n",
      "\tTest Loss =  0.08478846004017046  Test Accuracy =  0.74682\n",
      "Trying degree 1 with lambda = 0.21544346900318834 :\n",
      "\tTraining Loss =  0.08442022939899789\n",
      "\tTest Loss =  0.08478845634283773  Test Accuracy =  0.74682\n",
      "Trying degree 1 with lambda = 4.6415888336127775 :\n",
      "\tTraining Loss =  0.0844202313045559\n",
      "\tTest Loss =  0.0847883932383634  Test Accuracy =  0.74684\n",
      "Trying degree 1 with lambda = 100.0 :\n",
      "\tTraining Loss =  0.08442095272637003\n",
      "\tTest Loss =  0.08478796937293047  Test Accuracy =  0.7467\n",
      "Trying degree 2 with lambda = 0.01 :\n",
      "\tTraining Loss =  0.07885540126223511\n",
      "\tTest Loss =  0.07898383884311637  Test Accuracy =  0.77568\n",
      "Trying degree 2 with lambda = 0.21544346900318834 :\n",
      "\tTraining Loss =  0.07885540167887552\n",
      "\tTest Loss =  0.0789838286886328  Test Accuracy =  0.77568\n",
      "Trying degree 2 with lambda = 4.6415888336127775 :\n",
      "\tTraining Loss =  0.07885540650615881\n",
      "\tTest Loss =  0.07898363320051156  Test Accuracy =  0.77572\n",
      "Trying degree 2 with lambda = 100.0 :\n",
      "\tTraining Loss =  0.07885744139889607\n",
      "\tTest Loss =  0.07898212686589384  Test Accuracy =  0.77554\n"
     ]
    }
   ],
   "source": [
    "rr_models, rr_losses, rr_accuracies, rr_preds =  ridge_trials(y, tx, tx_sub, (1,3),(-2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rr = rr_preds[np.argmax(rr_accuracies)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(ids_sub, best, \"predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
