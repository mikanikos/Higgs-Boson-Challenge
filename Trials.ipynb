{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from proj1_helpers import *\n",
    "from costs import *\n",
    "from helpers import *\n",
    "from implementations import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_trials(y, tx, tx_sub, degree_range, lambda_range, partitions=2):\n",
    "    ## Split data into test and training sets\n",
    "    ## If partitions > 2, use k-fold cross-validation\n",
    "    glob_tx_tr, glob_tx_te, glob_y_tr, glob_y_te = split_data(tx, y, 0.8)\n",
    "\n",
    "    ## Initial results: losses, weights, preditions and (test) losses\n",
    "    models = []\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    predictions = []\n",
    "    \n",
    "    ## Loops over range of degrees\n",
    "    degrees = range(degree_range[0], degree_range[1])\n",
    "    lambdas = np.logspace(lambda_range[0], lambda_range[1], num=1+(lambda_range[1]-lambda_range[0]))\n",
    "    for degree in degrees:\n",
    "        ## Loops over range of lambdas\n",
    "        for lambda_ in lambdas:\n",
    "            print(\"Trying degree\", degree,\"with lambda =\", lambda_,\":\")\n",
    "\n",
    "            tx_tr, tx_te, tx_pred = expand(degree, glob_tx_tr, glob_tx_te, tx_sub)\n",
    "\n",
    "            w, loss = ridge_regression(glob_y_tr, tx_tr, lambda_)\n",
    "            print(\"\\tTraining Loss = \", loss)\n",
    "\n",
    "            y_test = predict_labels(w, tx_te)\n",
    "            test_loss = compute_loss(glob_y_te, tx_te, w)\n",
    "            accuracy = compute_accuracy((y_test+1)/2, glob_y_te)\n",
    "            y_pred = predict_labels(w, tx_pred)\n",
    "\n",
    "            print(\"\\tTest Loss = \", test_loss, \" Test Accuracy = \", accuracy )\n",
    "            models.append((\"ridge_regression\", degree, lambda_, w))\n",
    "            losses.append(test_loss)\n",
    "            accuracies.append(accuracy)\n",
    "            predictions.append(y_pred)\n",
    "    return models, losses, accuracies, predictions\n",
    "    \n",
    "MAX_ITERS = 100  \n",
    "GAMMA = 0.6\n",
    "\n",
    "## Performs logistic trials over set of hyper-parameters (degrees)\n",
    "## Results result from these trials with corresponding test losses\n",
    "def logistic_trials(y, tx, tx_sub, degree_range, partitions=2):\n",
    "    ## Split data into test and training sets\n",
    "    ## If partitions > 2, use k-fold cross-validation\n",
    "    glob_tx_tr, glob_tx_te, glob_y_tr, glob_y_te = split_data(tx, y, 0.8)\n",
    "\n",
    "    ## Initial results: losses, weights, preditions and (test) losses\n",
    "    models = []\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    predictions = []\n",
    "    \n",
    "    ## Loops over range of degrees\n",
    "    degrees = range(degree_range[0], degree_range[1])\n",
    "    for degree in degrees:\n",
    "        print(\"Trying degree\", degree, \":\")\n",
    "\n",
    "        tx_tr, tx_te, tx_pred = expand(degree, glob_tx_tr, glob_tx_te, tx_sub)        \n",
    "        initial_w = np.ones(tx_tr.shape[1])\n",
    "        \n",
    "        w, loss = logistic_regression(glob_y_tr, tx_tr, initial_w, MAX_ITERS, GAMMA)\n",
    "        print(\"\\tTraining Loss = \", loss)\n",
    "        \n",
    "        y_test = predict_labels(w, tx_te)\n",
    "        test_loss = compute_loss(glob_y_te, tx_te, w, func=\"logistic\")\n",
    "        accuracy = compute_accuracy((y_test+1)/2, glob_y_te)\n",
    "        y_pred = predict_labels(w, tx_pred)\n",
    "\n",
    "        print(\"\\tTest Loss = \", test_loss, \" Test Accuracy = \", accuracy )\n",
    "        models.append((\"logistic_SGD\", degree, w))\n",
    "        losses.append(test_loss)\n",
    "        accuracies.append(accuracy)\n",
    "        predictions.append(y_pred)\n",
    "    return models, losses, accuracies, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Because expansion and standardization are transformations of our initial feature set\n",
    "## We must apply identical transformations to all feature sets we wish to make predictions upon\n",
    "def expand(degree, tx_tr, tx_te, tx_pred):\n",
    "    ## Extract jet numbers as three indicator variables\n",
    "    ## Remove them so they will not be standardized or expanded\n",
    "    jets_tr = jet_nums(tx_tr)\n",
    "    jets_te= jet_nums(tx_te)\n",
    "    jets_pred= jet_nums(tx_pred)\n",
    "    ## Remove redundant columns\n",
    "    res_tr = extract_col(tx_tr)\n",
    "    res_te = extract_col(tx_te)\n",
    "    red_pred = extract_col(tx_pred)\n",
    "    ## Expand features to include polynomial terms\n",
    "    res_tr = build_poly(tx_tr, degree)\n",
    "    res_te = build_poly(tx_te, degree)\n",
    "    res_pred = build_poly(tx_pred, degree)\n",
    "    ## Standardize\n",
    "    res_tr, mean, std = standardize(res_tr)\n",
    "    res_te = (res_te-mean)/std\n",
    "    res_pred = (res_pred-mean)/std\n",
    "    ## Fix NaNs resulting from division by 0\n",
    "    res_tr[np.isnan(res_tr)]=1\n",
    "    res_te[np.isnan(res_te)]=1\n",
    "    res_pred[np.isnan(res_pred)]=1\n",
    "    ## Reconcatenate jet indicator columns\n",
    "    res_tr = np.c_[res_tr, jets_tr]\n",
    "    res_te = np.c_[res_te, jets_te]\n",
    "    res_pred = np.c_[res_pred, jets_pred]\n",
    "    return res_tr, res_te, res_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tx(tx):\n",
    "    result = np.copy(tx)\n",
    "    ## Convert -999 to NaN as we believe these are misidentified data\n",
    "    ## Recording as NaN prevents them from influencing nanmean calculations\n",
    "    result[result==-999]=np.nan\n",
    "    \n",
    "    ## Now replace each NaN index with the mean of its column\n",
    "    means = np.nanmean(result, axis=0)\n",
    "    nans = np.where(np.isnan(result))\n",
    "    result[nans] = np.take(means, nans[1])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Jet number seems to be categorical, taking on three discrete values\n",
    "## Relative values do not seem to have meaning, so coefficients are not a good way to treat this\n",
    "## Solution: Split this into three indicator vectors. Each indicator takes a different coefficient\n",
    "def jet_nums(tx):\n",
    "    jets = tx[:,22]\n",
    "    new_tx = np.delete(tx, 22, axis=1)\n",
    "    jet0 = np.zeros((jets.shape[0],1))\n",
    "    jet0[jets==0] = 1\n",
    "    jet1 = np.zeros((jets.shape[0],1))\n",
    "    jet1[jets==1] = 1\n",
    "    jet2 = np.zeros((jets.shape[0],1))\n",
    "    jet2[jets==2] = 1\n",
    "    jet3 = np.zeros((jets.shape[0],1))\n",
    "    jet3[jets==3] = 1\n",
    "    result = np.c_[jet0, jet1, jet2, jet3]\n",
    "    return result\n",
    "\n",
    "def extract_col(tx):\n",
    "    result = np.delete(tx, 22, axis=1)\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training sets\n",
    "y, tx, ids = load_csv_data(\"data/train.csv\")\n",
    "\n",
    "## For training, let y exist on {0, 1} rather than {-1, 1} to improve cost calculations\n",
    "y = (y + 1) / 2\n",
    "\n",
    "## Fix issues with dataset involving suspect outliers\n",
    "tx = clean_tx(tx)\n",
    "\n",
    "## Load submission dataset\n",
    "y_sub, tx_sub, ids_sub = load_csv_data(\"data/test.csv\")\n",
    "tx_sub = clean_tx(tx_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying degree 1 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marshallcooper/ML/proj1/MachineLearning_Project1/helpers.py:39: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  x = x/stds\n",
      "/Users/marshallcooper/ML/proj1/MachineLearning_Project1/helpers.py:39: RuntimeWarning: invalid value encountered in true_divide\n",
      "  x = x/stds\n",
      "/Users/marshallcooper/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/marshallcooper/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/marshallcooper/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/marshallcooper/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss =  3.845498660169477e-08\n",
      "\tTest Loss =  nan  Test Accuracy =  0.63436\n",
      "Trying degree 2 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marshallcooper/ML/proj1/MachineLearning_Project1/costs.py:20: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = y.T.dot(np.log(pred)) + (1 - y).T.dot(np.log(1 - pred))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss =  5.1733383021348905e-05\n",
      "\tTest Loss =  nan  Test Accuracy =  0.62944\n",
      "Trying degree 3 :\n",
      "\tTraining Loss =  7.320090438504591\n",
      "\tTest Loss =  nan  Test Accuracy =  0.6658\n"
     ]
    }
   ],
   "source": [
    "models, losses, accuracies, preds =  logistic_trials(y, tx, tx_sub, (1,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_logsgd = preds[np.argmax(accuracies)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1., -1., -1., ...,  1., -1., -1.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_logsgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying degree 7 with lambda = 0.01 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marshallcooper/ML/proj1/MachineLearning_Project1/helpers.py:39: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  x = x/stds\n",
      "/Users/marshallcooper/ML/proj1/MachineLearning_Project1/helpers.py:39: RuntimeWarning: invalid value encountered in true_divide\n",
      "  x = x/stds\n",
      "/Users/marshallcooper/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/marshallcooper/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/marshallcooper/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/marshallcooper/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss =  0.0723559123960287\n",
      "\tTest Loss =  0.07498035939430843  Test Accuracy =  0.80082\n",
      "Trying degree 7 with lambda = 0.1778279410038923 :\n",
      "\tTraining Loss =  0.07339981408960228\n",
      "\tTest Loss =  0.07405314214841499  Test Accuracy =  0.79726\n",
      "Trying degree 7 with lambda = 3.1622776601683795 :\n",
      "\tTraining Loss =  0.07387292435457403\n",
      "\tTest Loss =  0.07433472669753109  Test Accuracy =  0.79602\n",
      "Trying degree 7 with lambda = 56.23413251903491 :\n",
      "\tTraining Loss =  0.07452479379977583\n",
      "\tTest Loss =  0.07490395262337601  Test Accuracy =  0.7934\n",
      "Trying degree 7 with lambda = 1000.0 :\n",
      "\tTraining Loss =  0.07614689821873045\n",
      "\tTest Loss =  0.07703775381349459  Test Accuracy =  0.78494\n"
     ]
    }
   ],
   "source": [
    "rr_models, rr_losses, rr_accuracies, rr_preds =  ridge_trials(y, tx, tx_sub, (7,8),(-2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rr = rr_preds[np.argmax(rr_accuracies)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(ids_sub, best, \"predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
